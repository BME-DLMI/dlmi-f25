{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Lecture 05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "The goal of this assignment is to go through the data-driven approach process with a linear classifer model. This will include loading a dataset, preprocessing, and splitting a dataset into training/validation/testing splits, training a linear classifier, model evaluation, and visualazation. We will use a toy dataset called MedNIST, which consists of toy medical images of different modalities and organs. The goal is to classify each image into 6 catergories: Abdomen CT, Breast MRI, Chest XRay, Chest CT, Hand XRay, and Head CT. The images are 2D and significantly downsampled to $64 x 64$ pixels. For simplicity and efficiency, for this in-class activity we will work with a sample of 1000 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "engr_dir = \"/opt/nfsopt/DLMI\"\n",
    "idas_dir = os.path.join(os.path.expanduser('~'), \"classdata\")\n",
    "\n",
    "if os.path.isdir(engr_dir):\n",
    "    data_dir = engr_dir\n",
    "elif os.path.isdir(idas_dir):  \n",
    "    data_dir = idas_dir\n",
    "else:\n",
    "    print(\"Data directory not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "The `MEdNIST.csv` file is a text file with the image filenames and corresponding class label for each sample in the dataset. Below the csv file is read using the `pandas` library, and the first 10 lines of the file are displayed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_dir,\"lecture05\", \"MedNIST.csv\"))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Define a Python dictionary which maps the numerical class value to the corresponding category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {0: 'AbdomenCT',\n",
    "               1: 'BreastMRI',\n",
    "               2: 'CXR',\n",
    "               3: 'ChestCT', \n",
    "               4: 'Hand',\n",
    "               5: 'HeadCT'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Read and store all the images in a single 2D matrix $X$. Each image will be flattened and stored as a row in $X$. The size of $X$ with be $N \\times M$ where N is the number of samples and M is the number of pixels in each image. \n",
    "The spatial structure of the image will be lost after flattening the image, however, the spatial structure is not used when applying a linear classification model to raw pixel values.\n",
    "Storing the data in a single matrix will allow us to compute linear classification scores more efficiently. The labels for each image will be stored in a single vector $Y$ with $N$ elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "First determine the size of the dataset, i.e., the number of samples (N) and numper of pixels per image (M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_filename = os.path.join(data_dir, \"lecture05\", df.iloc[0]['filename'])\n",
    "image_width, image_height = PIL.Image.open(full_filename).size\n",
    "N = len(df)\n",
    "M = image_width * image_height\n",
    "\n",
    "print(\"Image dimensions: {} x {}\".format(image_width, image_height))\n",
    "print(\"Number of samples (N): {}\".format(N))\n",
    "print(\"Number of pixels (M): {}\".format(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Create empty NumPy array to store dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((N, M))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Iterate over the rows of the dataframe, read each image, and store as row in data array. Additionally, create an array for the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate data array X\n",
    "for i, row in df.iterrows():\n",
    "    filename = row['filename']\n",
    "    label = row['label']\n",
    "    full_filename = os.path.join(data_dir, \"lecture05\", filename)\n",
    "    im = PIL.Image.open(full_filename)\n",
    "    arr = np.array(im)\n",
    "    X[i,:] = arr.ravel()\n",
    "\n",
    "# Create labels array Y\n",
    "Y = df['label'].values\n",
    "\n",
    "\n",
    "print(\"data shape: {}\".format(X.shape))\n",
    "print(\"label shape: {}\".format(Y.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**QUESTION:** For training a linear classifer on this dataset, what are learnable parameters and what are the corresponding sizes/shapes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "XXX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Visualization is an important step for exploring data and validating pre-processing steps. The code below will take a random sample and display the image with corresponding label. To view the image, the row vector needs to be reshaped into the 2D matrx using the calculated `image_width` and `image_height`. The code also displays the corresponding histogram below each image, so the instensity values and distrubution can be assessed. Run this cell multiple times to see different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 6, figsize=(15, 6))\n",
    "\n",
    "for i, k in enumerate(np.random.randint(N, size=6)):\n",
    "    arr = X[k,:]\n",
    "    label = Y[k]\n",
    "    arr2d = arr.reshape(image_width, image_height)\n",
    "    ax[0,i].set_title(class_names[label])\n",
    "    ax[0,i].imshow(arr2d, cmap=\"gray\", vmin=0, vmax=255)\n",
    "    ax[1,i].hist(arr, bins=20, range=(0, 255), edgecolor='black', color='white', density=True)\n",
    "    \n",
    "    ax[0,i].set_axis_off()\n",
    "    ax[1,i].set_yticklabels([])\n",
    "    ax[1,i].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Intensity Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Notice some images look washed. It is common to rescale image intensities to have values in the range $[-1, 1]$. To do this a linear rescaling will be used which maps the max intensity value to 1 and the minimum intensity to -1. To avoid potential outliers, we will use the 1st and 99th percentiles of the intensity distribution instead of the min and max. You could write a for loop to iterate over the rows of our data matrix X, and for each row calculate the 1st percentile and 99th percentile. However, NumPy provides functions which can do this in a more efficient way. For example, to calculate the 50th percentile of each image in our data matrix, we could call the np.percentile function, specify that we want to take the 50th perecentile (q=50), and specify the calculation to be performed across columns (axis=1). The result will be a percentile value for each row (corresponding to each image). Below complete the code to calculate the 1st percentile and 99th percentile for each image in the dataset (replace XXX)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "#######         TODO           #######\n",
    "######################################\n",
    "\n",
    "perc01 = XXX\n",
    "perc99 = XXX\n",
    "\n",
    "\n",
    "print(\"data shape: {}\".format(X.shape))\n",
    "print(\"perc01 shape: {}\".format(perc01.shape))\n",
    "print(\"perc99 shape: {}\".format(perc99.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "What are the shapes of the perc01 and perc99 arrays? What does each element of the arrays represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "XXX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "Now perform linear rescaling to map [perc01, perc99] intensities for each image to $[-1, 1]$. For an image $I$, the desired linear intensity rescaling is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "I_{norm} = 2\\frac{I - \\mathrm{perc_{01}}(I)}{\\mathrm{perc_{99}}(I)-\\mathrm{perc_{01}}(I)}-1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Similar to before, we could write a for loop over the rows of the data matrix of for each row perform this operation. However this is unneccessary because we can take advantage of NumPy broadcasing and vectorization which is significantly more efficient. Below, perform the linear rescaling on our data matrix $X$ without using a for loop (replace XXX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "#######         TODO           #######\n",
    "######################################\n",
    "\n",
    "X_norm = XXX\n",
    "\n",
    "print(X_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "WARNING: if you get the following error: \"operands could not be broadcast together with shapes (1000,4096) (1000,) (1000,)\"\n",
    "\n",
    "The perc01 and perc99 arrays are not broadcastable with the data array $X$. For arrays to be broadcastable, the sizes of each dimension must match or one must equal 1, this comparison is made starting at the right last dimension (last number returned by shape).\n",
    "\n",
    "For example, the shapes of the arrays are:\n",
    "\n",
    "|   |  |   |\n",
    "| :------- | :------: | -------: |\n",
    "| X | 1000, | 4096 |\n",
    "| perc01 |  | 1000 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Since the last dimensions (4096 and 1000) are not equal and neither equals 1, the arrays are not broadcastable. To fix this, a \"dummy\" dimension can be added as the last dimension of the perc01/perc99 arrays with size 1\n",
    "\n",
    "|   |  |   |\n",
    "| :------- | :------: | -------: |\n",
    "| X | 1000, | 4096 |\n",
    "| perc01 | 1000, | 1 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Now, the arrays are broadcastable since each dimension matches or equals 1. To fix this, try adding the parameter `keepdims=True` to your np.percentile calls, which will preserve the column dimension. \n",
    "\n",
    "More reading on broadcasting: https://numpy.org/doc/stable/user/basics.broadcasting.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Lastly, clip the intensities of the normalized array $X_{norm}$ to be between $[-1, 1]$. This will map values that were less than perc01 to -1, and values greater than perc99 to -1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = np.clip(X_norm, -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Now visualize the normalized dataset. Run this cell multiple times to see different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 6, figsize=(15, 6))\n",
    "\n",
    "for i, k in enumerate(np.random.randint(N, size=6)):\n",
    "    arr = X_norm[k,:]\n",
    "    label = Y[k]\n",
    "    arr2d = arr.reshape(image_width, image_height)\n",
    "    ax[0,i].set_title(class_names[label])\n",
    "    ax[0,i].imshow(arr2d, cmap=\"gray\")\n",
    "    ax[1,i].hist(arr, bins=20, edgecolor='black', color='white', density=True)\n",
    "    \n",
    "    ax[0,i].set_axis_off()\n",
    "    ax[1,i].set_yticklabels([])\n",
    "    ax[1,i].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "**QUESTION:** How have the images and intensity distributions changed after normalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "XXX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## Training / Validation / Testing Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Split the dataset into training, validation, testing sets. The total dataset is 1000 samples, we will use a 600 / 200 / 200 split. Each dataset will be saved as a `npz` which we can load into a NumPy array. Multiple arrays can be stored in a single `npz` file by specifying keyword argments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = int(N*0.6) # 60% training\n",
    "num_val = int(N*0.2) # 20% validation\n",
    "num_test = N - num_train - num_val # remainder tresting\n",
    "\n",
    "Xtrain, Xval, Xtest = X_norm[:num_train], X_norm[num_train:num_train+num_val], X_norm[num_train+num_val:]\n",
    "Ytrain, Yval, Ytest = Y[:num_train], Y[num_train:num_train+num_val], Y[num_train+num_val:]\n",
    "\n",
    "np.savez(\"train.npz\", X=Xtrain, Y=Ytrain)\n",
    "np.savez(\"val.npz\", X=Xval, Y=Yval)\n",
    "np.savez(\"test.npz\", X=Xtest, Y=Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Load the datasets. Since multiple arrays are saved in each `npz` file, the `np.load` call will return a Python dictionary, and each array will can be accessed with the keys corresponding to the keyword used when saving. For example. when loading \"train.npz\" into variable train, the data array can be accessed using train['X'] and the labels array can be accesed using train['Y']. Below the datasets are loaded and the array shapes are printed to confirm everythig was save/loaded properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load(\"train.npz\")\n",
    "test  =  np.load(\"test.npz\")\n",
    "val  =  np.load(\"val.npz\")\n",
    "\n",
    "print(\"Train X: {}\".format(train['X'].shape))\n",
    "print(\"Train Y: {}\".format(train['Y'].shape))\n",
    "print(\"Val X: {}\".format(val['X'].shape))\n",
    "print(\"Val Y: {}\".format(val['Y'].shape))\n",
    "print(\"Test X: {}\".format(test['X'].shape))\n",
    "print(\"Test Y: {}\".format(test['Y'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## Train Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "In the model.py file, a class called `LinearClassifier` is provided for training a softmax classifer. At this point in the class, you don't need to undertsand exactly what is going on in the code, if you are interested you can open the file. For this assignment, you will utilize the class for training a softmax classifer on your dataset. Below an instance of the `LinearClassifier` class is created, and the `train` member function is called with the training data and labels. The `train` function returns the learned model parameters `W` and `b`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import LinearClassifier\n",
    "\n",
    "clf = LinearClassifier(epochs = 1000,learning_rate=0.02)\n",
    "W, b = clf.train(train['X'],train['Y'])\n",
    "\n",
    "print(f'W.shape {W.shape}')\n",
    "print(f'b.shape {b.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "Below visualize the `W` parameters learned. For each of the 6 classes, the corresponding parameters are reshaped into an image. They weights can be thought of as the \"templates\" learned for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,6, figsize = (12, 8),constrained_layout=True)\n",
    "for i in range(6):\n",
    "    axs[i].imshow(W.T[i].reshape(64,64), cmap = \"gray\")\n",
    "    axs[i].axis('off')\n",
    "    axs[i].set_title(class_names[i])\n",
    "plt.savefig(\"svm_w.pdf\",bbox_inches='tight', pad_inches=.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "Implement the linear classifer predict function using the learned parameters `W` and `b`. Additionally, the predict function requires input data `X` which it will predict the class labels for. Your code should work for X with multiple samples ($N \\times M$ matrix) . Note, use vectorized code NOT for loops!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W, b):\n",
    "    \"\"\"\n",
    "    Linear classifer prediction \n",
    "\n",
    "    Assume the input is N samples organized in a 2D matrix of N x M\n",
    "\n",
    "    Inputs:\n",
    "    - X (np.array): NumPy array of data to predict labels, size N x M\n",
    "    - W (np.array): NumPy array of learned model weights\n",
    "    - b (np.array): NumPy array of learned model biases\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    - y (np.array): NumPy array of predictions for each sample, size N\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    #######         TODO           #######\n",
    "    ######################################\n",
    "\n",
    "    return y\n",
    "\n",
    "def accuracy(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "    Calculates accuracy \n",
    "\n",
    "    Inputs:\n",
    "    - Y_true (np.array): NumPy array true class labels\n",
    "    - Y_pred (np.array): NumPy array predicted class labels\n",
    "   \n",
    "\n",
    "    Returns:\n",
    "    - acc (double): Accuracy as percentage\n",
    "    \"\"\"\n",
    "    \n",
    "    ######################################\n",
    "    #######         TODO           #######\n",
    "    ######################################\n",
    "\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "Predict the labels and report the accuracy on the training, validation, and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "#######         TODO           #######\n",
    "######################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
