{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Lecture 07"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In this assignmnt you will design and train a linear classifier and a multilayer perceptron (MLP) on a simple dataset, and compare the results. The models should be implemented from \"scratch\" using PyTorch low-level tensor operations (i.e,. without the convenience of `torch.nn` module). In future lectures, we will transition to using the high-level PyTorch `torch.nn` module. In practice, you will likely use the `torch.nn` module frequently for existing layer types, however, it is valuable to know how to program in PyTorch using tensors as this will be necessary for implementing novel layer types, loss functions, models that are not implementd in the `nn.module`. Additionally, this practice will reinforce your theoretical understanding which is easy to bypass using the abstract building blocks defined in `nn.module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummary import summary\n",
    "from utils import callback_plot, create_gif\n",
    "\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS is available.\")\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"device: {}\".format(device))\n",
    "\n",
    "LIN_OUTPUT_DIR = \"./linear_vizualizations/\"\n",
    "if not os.path.exists(LIN_OUTPUT_DIR):\n",
    "    os.mkdir(LIN_OUTPUT_DIR)\n",
    "MLP_OUTPUT_DIR = \"./mlp_vizualizations/\"\n",
    "if not os.path.exists(MLP_OUTPUT_DIR):\n",
    "    os.mkdir(MLP_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "In this assignment, you will use a simple toy dataset for binary classification. The data is stored in a matrix of size $N\\times M$ where $N$ is the number of samples and $M$ is the number of features, in this case $N=1000$ and $M=2$. The labels are stored in a vector of size $N$, with values 0 or 1 corresponding to the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 1000\n",
    "TEST_SIZE = 100 \n",
    "\n",
    "X, Y = make_moons(n_samples = N_SAMPLES, noise=0.2, random_state=100)\n",
    "X = X.astype(np.float32)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=TEST_SIZE, random_state=42)\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"Y_train shape: {}\".format(Y_train.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"Y_test shape: {}\".format(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "<br>\n",
    "Visualize the datset using a matplotlib scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax.scatter(X_train[:,0],X_train[:,1], c=Y_train, cmap=plt.cm.coolwarm, edgecolors='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## NumPy to torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "PyTorch tensors are very similar to numpy arrays, except tensors can seemlessly run on GPUs for accelerated computing. Additionally, PyTorch tensors are also optimized for automatic differentiation (autograd), greatly simiplifying neural network backpropagation. You will build a MLP using `torch.tensor` objects to store all matrices (data, model parameters, intermediate  layer activations) and perform calculations. Conventiently the API of PyTorch and NumPy are almost identical. For example `torch.sum(x, dim=1, keepdim=True)` and `np.sum(x, dim=1, keepdim=True)` perform the same operation, the only difference is torch operates on `torch.tensor` objects whereas NumPy operates on `np.array` objects. \n",
    "Our dataset is currently stored as `np.array`, so first we convert to PyTorch tensors and then transfer them to the desired computing device. Similar to numpy, we can access the size of the tensor using the shape attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(X_train).to(device)\n",
    "labels = torch.from_numpy(Y_train).to(device)\n",
    "\n",
    "inputs_test = torch.from_numpy(X_test).to(device)\n",
    "labels_test = torch.from_numpy(Y_test).to(device)\n",
    "\n",
    "print(\"train inputs shape: :{}\".format(inputs.shape))\n",
    "print(\"train labels shape: :{}\".format(labels.shape))\n",
    "\n",
    "print(\"test inputs shape: :{}\".format(inputs_test.shape))\n",
    "print(\"test labels shape: :{}\".format(labels_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Part 1: Linear Classifier with torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Recall the output of a linear classifer is calculated as $z = Wx + b$ where $W\\in R^{K\\times M}$, $x\\in R^M$, $b \\in R^{K}$ and the output $z\\in R^{K}$. In practice, this operation is performed in parallel on a **batch** of data, or a subset of samples. Intead of $x$ being a vector representing a single sample, it will be a matrix where $x \\in R^{N\\times M}$ for a batch of $N$ samples, where each row represents a different samle. For the dimensions to be valid for matrix multiplication, the equation needs to be manipulated slightly. Assuming the input is $x \\in R^{N\\times M}$ and the output is $z \\in R^{N\\times K}$, write the equation for a linear classifer (in the batch setting) and specify the size of the weights and biases in the markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "XXX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Below you will define a `LinearClassifier` class to represent a simple linear classifier. The `__init__` function should initialize the model parameters (weights and biases) using PyTorch tensors. The weights should be intitalized with random values using `torch.randn` and the biases can be be italized to zeros using `torch.zeros`. For example, `torch.randn(4,8, device=device)` will create random matrix with size $4 \\times 8$. `torch.randn` will generate random values from a standard normal distribution (i.e. mean 0, standard deviation 1). Ensure all weights and biases have gradient tracking enabled by calling the `requires_grad_()` method on all tensors.\n",
    "\n",
    "\n",
    "The `__call__` function should impement the forward pass (evaluate the linear classifier equation), using the input data $X \\in R^{N\\times M}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier:\n",
    "    \"\"\"\n",
    "    A two-hidden layer multilayer perceptron \n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    - input_size (int): number of input features (M)\n",
    "    - output_size (int): number of output classe (K)\n",
    "    \n",
    "    Attributes:\n",
    "    - W1 (torch.tensor): weight matrix \n",
    "    - b1 (torch.tensor): bias vector \n",
    "\n",
    "    Methods:\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        ######################################\n",
    "        #######         TODO           #######\n",
    "        ######################################\n",
    "        # replace XXX and create the remaining parameters\n",
    "        self.W1 = torch.randn(XXX, XXX, device = device)\n",
    "        self.W1.requires_grad_()\n",
    "        \n",
    "\n",
    "    def __call__(self, X):\n",
    "        ######################################\n",
    "        #######         TODO           #######\n",
    "        ######################################\n",
    "        # implement forward pass\n",
    "        \n",
    "        return z1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "By implementing the `__call__` function, an object of the LinearClassifer class can be treated as a callable instance. In other words, the LinearClassifier instance behaves similar to calling a function. For example the below code first creates an instance of the `LinearClassifier` class and next the forward pass is executed (by executing the code in `__call__`)\n",
    "\n",
    "`mymodel = LinearClassifier(2,2)`\n",
    "\n",
    "`logit = mymodel(input_data)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Next, implment the softmax function using torch operations (not NumPy). The input to the softmax function are the predicted logits, and the output is the predicted probabilites. The softmax function will be called by the categorical_cross_entropy loss function which is defined below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Calculates the softmax on input x with size NxC\n",
    "\n",
    "    N is the number of samples\n",
    "\n",
    "    C is the number of classes\n",
    "\n",
    "    Each row of the output will be a valid probability distribiution\n",
    "    i.e., all values between 0 and 1, and each row sums to 1\n",
    "\n",
    "    Inputs:\n",
    "    - x (torch.tensor): tensor with size NxC\n",
    "\n",
    "    Returns:\n",
    "    - p (torch.tensor): tensor with size NxC\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    #######         TODO           #######\n",
    "    ######################################\n",
    "    e_x = XXX\n",
    "    p = XXX\n",
    "    \n",
    "    return p\n",
    "\n",
    "def categorical_cross_entropy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Calculates the categorical cross entropy.\n",
    "\n",
    "    y_pred are logits and y_true are class labels\n",
    "\n",
    "    The function first calcultes softmax of the logits\n",
    "    and then computes the cross entropy \n",
    "\n",
    "    Inputs:\n",
    "    - y_pred (torch.tensor): tensor of logits with size NxC\n",
    "    - y_true (torch.tensor): tensor with size N,\n",
    "\n",
    "    Returns:\n",
    "    - loss (torch.tensor): scalar loss (average cross entropy over samples)\n",
    "    \"\"\"\n",
    "    N = y_pred.shape[0]\n",
    "    y_prob = softmax(y_pred)\n",
    "    \n",
    "    cross_entropy = -torch.log(y_prob[torch.arange(N).long(), y_true.long()])\n",
    "    loss = torch.sum(cross_entropy) / N\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Below write the training loop. The training loop will iterate for 5000 epochs. Since this is a toy example and the network is small, we can pass the entire training dataset through the model each epoch (rather than sampling batches). For each epoch the following steps are performed: forward pass, calculate loss, backward pass, update weights. To perform the backward pass you can call `loss.backward()`, where loss is the tensor returned by the `categorical_cross_entropy` function. The `backward()` functionality is available since we are using PyTorch tensors, this is the convenient autograd feature. After calling backward, you can access the gradient of all variables using the `grad` attribute, i.e. `W1.grad`. Implement gradient descent to update the values of all the weights and biases in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "#######         TODO           #######\n",
    "######################################\n",
    "\n",
    "# TODO: initialize model by creating instance of LinearClassifier class\n",
    "# with two input features, and two ouputs\n",
    "model = XXX\n",
    "\n",
    "# Set hyperparameters for training\n",
    "lr = 0.01 # learning rate/step size for gradient descent\n",
    "epochs = 5000 \n",
    "\n",
    "# Set the loss function\n",
    "loss_func = categorical_cross_entropy # loss function\n",
    "\n",
    "# Create empty list to store loss value each epoch\n",
    "history = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # TODO: forward pass to calculate logits\n",
    "    logit = XXX\n",
    "\n",
    "    # calculate loss \n",
    "    loss = loss_func(logit, labels)\n",
    "    \n",
    "    history.append(loss.item())\n",
    "    \n",
    "    # Backward pass to update gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Exclude weight update from computation graph\n",
    "    with torch.no_grad(): \n",
    "       \n",
    "        # TODO: update weights using gradient descent on all parameters. \n",
    "        # gradient of W1: model.W1.grad \n",
    "       \n",
    "       \n",
    "        # After updating the weights, clear the gradients for the next epoch\n",
    "        model.W1.grad.zero_()\n",
    "        model.b1.grad.zero_()\n",
    "       \n",
    "        # Plot decision boundary\n",
    "        if(epoch % 50 == 0):\n",
    "            callback_plot(epoch, X_train, Y_train, model, device, LIN_OUTPUT_DIR)\n",
    "\n",
    "\n",
    "# Plot loss per epoch\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(history)\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "The code below will create a gif of the training visualizations. After running the code, open the linear.gif file to visualize the classification decision boundary changing the model parameters update each epoch during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_gif_path = \"linear.gif\"\n",
    "image_paths = sorted(glob.glob(LIN_OUTPUT_DIR+\"/*\"))\n",
    "create_gif(image_paths, output_gif_path, duration=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Calculate the accuracy of the model on the test dataset (inputs_test and labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    preds = torch.argmax(y_pred, dim=1)\n",
    "    return (preds == y_true).float().mean()\n",
    "\n",
    "######################################\n",
    "#######         TODO           #######\n",
    "######################################\n",
    "\n",
    "# TODO: get predictons on test dataset and print accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Part 2: MLP with torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Now consider a 2-layer MLP: a network with are two hidden layers, each followed by a ReLU nonlinearity. The network has the following form:\n",
    "\n",
    "$x \\rightarrow \\boxed{fc1} \\rightarrow z1 \\rightarrow \\boxed{ReLU} \\rightarrow a1 \\rightarrow \\boxed{fc2} \\rightarrow z2 \\rightarrow \\boxed{ReLU} \\rightarrow a2 \\rightarrow \\boxed{fc3}  \\rightarrow ouptut$\n",
    "\n",
    "where the boxes represent mathematical operations, and non-boxed values represent input and output tensors of each operation. \n",
    "\n",
    "In the markdown cell below, write the equaton of the 2-layer MLP above and specify the size of all weights and biases (equation and sizes should be appropirate for batch setting). Assume the input is $x \\in R^{N\\times M}$ and the output is $z \\in R^{N\\times K}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "XXX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Below you will define the `MLP` class to represent the 2-hidden layer MLP above. The `__init__` function should initialize the model parameters (weights and biases) using PyTorch tensors. The weights should be intitalized with random values using `torch.randn` and the biases can be be italized to zeros using `torch.zeros`. Improved training dynamics can be achieved by instead initalizing parameters with a standard deviation of $\\sqrt(2.0 / n)$ where $n$ is the number inputs to the layer, this is known as \"He Initialization\". Note, `torch.randn` uses a standard deviation of 1. Ensure all weights and biases have gradient tracking enabled by calling the `requires_grad_()` method on all tensors.\n",
    "\n",
    "\n",
    "The `__call__` function should impement the forward pass, using the input data $X \\in R^{N\\times M}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    A two-hidden layer multilayer perceptron \n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    - input_size (int): number of input features (M)\n",
    "    - h1_size (int): number of neurons in first hiddlen layer\n",
    "    - h2_size (int): number of neurons in second hiddlen layer\n",
    "    - output_size (int): number of output classe (K)\n",
    "    \n",
    "    Attributes:\n",
    "    - W1 (torch.tensor): weight matrix from input to hidden layer 1\n",
    "    - b1 (torch.tensor): bias vector for hidden layer 1\n",
    "    - W2 (torch.tensor): weight matrix from hidden layer 1 to hidden layer 2\n",
    "    - b2 (torch.tensor): bias vector for hidden layer 2\n",
    "    - W3 (torch.tensor): weight matrix from hidden layer 2 to output\n",
    "    - b3 (torch.tensor): bias vector for output\n",
    "\n",
    "    Methods:\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, h1_size, h2_size, output_size):\n",
    "        \n",
    "        ######################################\n",
    "        #######         TODO           #######\n",
    "        ######################################\n",
    "        # create all parameters\n",
    "       \n",
    "        \n",
    "\n",
    "    def __call__(self, X):\n",
    "        \n",
    "        ######################################\n",
    "        #######         TODO           #######\n",
    "        ######################################\n",
    "        # implement forward pass\n",
    "\n",
    "      \n",
    "        return z3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "By implementing the `__call__` function, an object of the MLP class can be treated as a callable instance. In other words, the MLP instance behaves similar to calling a function. For example the below code first creates an instance of the `MLP` class and next the forward pass is executed (by executing the code in `__call__`)\n",
    "\n",
    "`mymodel = MLP(2,8,16,2)`\n",
    "\n",
    "`logit = mymodel(input_data)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Below write the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "#######         TODO           #######\n",
    "######################################\n",
    "\n",
    "# TODO: initialize model by creating instance of MLP class\n",
    "# You can choose the sizes for hidden layers\n",
    "\n",
    "\n",
    "# Set hyperparameters for training\n",
    "lr = 0.01 # learning rate/step size for gradient descent\n",
    "epochs = 5000 \n",
    "\n",
    "# Set the loss function\n",
    "loss_func = categorical_cross_entropy # loss function\n",
    "\n",
    "# Create empty list to store loss value each epoch\n",
    "history = []\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # TODO: forward pass to calculate logits\n",
    "\n",
    "    \n",
    "    # TODO: calculate loss (assign to variable named loss)\n",
    "\n",
    "    \n",
    "    history.append(loss.item())\n",
    "    \n",
    "    # Backward pass to update gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Exclude weight update from computation graph\n",
    "    with torch.no_grad(): \n",
    "\n",
    "        # TODO: update weights using gradient descent on all parameters. \n",
    "        # gradient of W1: model.W1.grad \n",
    "        \n",
    "        \n",
    "        # After updating the weights, clear the gradients for the next epoch\n",
    "        model.W1.grad.zero_()\n",
    "        model.b1.grad.zero_()\n",
    "        model.W2.grad.zero_()\n",
    "        model.b2.grad.zero_()\n",
    "        model.W3.grad.zero_()\n",
    "        model.b3.grad.zero_()\n",
    "\n",
    "        # Plot decision boundary\n",
    "        if(epoch % 50 == 0):\n",
    "            callback_plot(epoch, X_train, Y_train, model, device, MLP_OUTPUT_DIR)\n",
    "\n",
    "\n",
    "# Plot loss per epoch\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(history)\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "The code below will create a gif of the training visualizations. After running the code, open the mlp.gif file to visualize the classification decision boundary changing the model parameters update each epoch during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_gif_path = \"mlp.gif\"\n",
    "image_paths = sorted(glob.glob(MLP_OUTPUT_DIR+\"/*\"))\n",
    "create_gif(image_paths, output_gif_path, duration=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "Lastly, calculate the accuracy of the model on the test dataset (inputs_test and labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################################\n",
    "#######         TODO           #######\n",
    "######################################\n",
    "\n",
    "# TODO: get predictons on test dataset and print accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Lastly, compare the results for the linear classifier and the MLP. Include a comparison of the decision boundaries learned by the two models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "XXX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
